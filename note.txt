# Idea:
L'idea del progetto è di implementare un semplice sistema di federated learning, composto da:
- un aggregator server;
- una decina di client.

Come usuale, il server manda l'architettura e i pesi iniziali della NN ai client che,
allenando il modello locale su una partizione del dataset, invieranno indietro al server il modello aggiornato.

L'idea per prevenire poisoning attacks e nodi bizantini, è quella di implementare un semplice algoritmo di consensus.
Il client, prima di mandare il modello aggiornato al server, lo manda ad altri K client. 
Questi dovranno votare il modello candidato, sulla base di metriche di similarità.
Se la maggioranza dei voti sarà positiva, il modello verrà mandato al server.

L'obiettivo finale del progetto è quello di effettuare esperimenti sulle metriche da utilizzare (similarità: euclidean distance [40], [52], cosine similarity [37], [38],
k-means [46], Pearson correlation coefficient [49], [51], etc.),
nonchè sul numero di client a cui mandare il modello (quanti client "validano" il modello locale di una macchina? Tutti? Solo quelli del sottoinsieme corrente? Uno solo del sottoinsieme corrente?), sulle varianti del protocollo di consensus e,
infine, su estensioni del sistema che utilizzino non solo metriche di similarità, 
ma anche metriche di performance, BRA (Krum aggregation,... ) e verification-based methods (vedi SVM e altri).

Nota: come evitare che il client modifichi il modello prima di mandarlo? magari il modello può essere cacheato dagli altri client e mandato da uno degli altri client in modo random, oppure può firmare con l'hash il risultato prima di inviarlo.

## Datasets:
l'idea iniziale era di utilizzare un dataset su malware android, che ben si sposa col progetto dato che:
- federated learning si può avere su un sistema distribuito nell'edge, quindi composto (o componibile) da smartphones;
- un attaccante potrebbe essere interessato a scalfire l'accuracy del modello per far passare un malware (magari in suo possesso) come goodware.
Inoltre, è sufficientemente grande da poterlo dividere tra i vari client.

Link dataset: https://archive.ics.uci.edu/dataset/855/tuandromd+(tezpur+university+android+malware+dataset)
Paper: https://www.semanticscholar.org/paper/97e4780810c69604f6e2f2f4e31b88bbef7ca669

Tuttavia, il dataset in questione è incompleto e sbilanciato nelle classi, per cui non è semplice allenare un modello in modo unbiased e ottenere delle buone prestazioni di partenza.
Per questo motivo, ho pensato di aggiungere un livello di astrazione, allenando un modello semplice sul dataset MNIST, che è ben conosciuto in letteratura e non richiede data preprocessing.
Modello e dataset sono in questo caso attori intercambiabili. Una volta che l'algoritmo è corretto, si può trovare un dataset più interessante e allenarvi sopra un modello più complesso.

## Result #0:
Ho preparato un sistema di ML classico sul dataset MNIST, con un modello MLP con 1 hidden layer di 512 neuroni.
Ho definito funzioni e procedure per allenare il modello sul dataset, effettuare una leggera model selection (ho ridotto al minimo il numero di parametri inseriti nella grid search: un paio di valori per le epoche,
per la batch-size, per il numero di neuroni nell'hidden layer e il learning rate), ri-allenare il modello scelto sull'intero training set e infine effettuare una valutazione del modello allenato.
Seguendo questo approccio, la model evaluation sul test set restituisce un'accuracy dello 0.9779 che, considerata l'architettura semplice del modello, i pochi parametri su cui sono state valutate le performances,
il basso numero di epoche e il mancato utilizzo di tecniche più avanzate (drop-out, RNN, weight-decay, ...) o altri tipi di ottimizzatori, è un ottimo risultato iniziale.
Il sistema in questione è stato scritto utilizzando PyTorch e Ray Tune.

## Result #1:
Ho replicato un semplice sistema di Federated Learning su cui effettuare esperimenti.
La client selection avviene selezionando k <= n client in modo casuale, con k parametro scelto dall'utente.
La model aggregation viene effettuata calcolando la media tra i k modelli locali.

Come prima sperimentazione, ho istanziato diversi numeri di client su un sistema di apprendimento a round singolo.
Un test sull'affidabilità e la replicabilità dei dati è stato effettuato su un sistema con un solo client, riportando l'accuracy attesa.
I risultati sono qui riportati:

Training set dinamici (real-time)
N. Round | N. Clients (tot) | Client selection per Round | Accuracy finale | N. Client Bizantini
    1           1                      100%                     0.9779               0
    1           1                      100%                     0.7674               1
    1           2                      100%                     0.9672               0
    1           2                      100%                     0.7678               1
    1           3                      100%                     0.9588               0
    1           4                      100%                     0.9511               0
    1           4                      80%                      0.9505               0
    1           4                      70%                      0.9510               0
    1           5                      100%                     0.9474               0
    1           5                      80%                      0.9463               0
    1           5                      70%                      0.9479               0
    1           6                      100%                     0.9425               0
    1           6                      80%                      0.9405               0
    1           6                      70%                      0.9421               0
    1           7                      100%                     0.9396               0
    1           8                      100%                     0.9377               0
    1           9                      100%                     0.9327               0
    1           10                     100%                     0.9291               0

    2           2                      100%                     0.9688               0
    2           3                      100%                     0.9621               0
    2           4                      100%                     0.9539               0
    2           4                      80%                      0.9515               0
    2           4                      70%                      0.9527               0
    2           5                      100%                     0.9495               0
    2           5                      80%                      0.9484               0
    2           5                      70%                      0.9493               0
    2           6                      100%                     0.9452               0
    2           6                      80%                      0.9439               0
    2           6                      70%                      0.9427               0

    3           2                      100%                     0.9678               0
    3           3                      100%                     0.9607               0
    3           4                      100%                     0.9537               0

    1           4                      100%                     0.9154               1
    1           4                      100%                     0.7508               2
    1           4                      100%                     0.7492               3
    1           4                       80%                     0.8630               1
    1           4                       80%                     0.7489               2
    1           4                       80%                     0.7466               3
    1           4                       70%                     0.7511               1
    1           4                       70%                     0.7511               2
    1           4                       70%                     0.7511               3
    1           5                      100%                     0.9277               1
    1           5                      100%                     0.7883               2
    1           5                      100%                     0.7462               3
    1           5                       80%                     0.9049               1
    1           5                       80%                     0.7506               2
    1           5                       80%                     0.7506               3
    1           5                       70%                     0.9455               1
    1           5                       70%                     0.8440               2
    1           5                       70%                     0.8572               3
    1           6                      100%                     0.9251               1
    1           6                      100%                     0.8570               2
    1           6                      100%                     0.7419               3
    1           6                       80%                     0.8997               1
    1           6                       80%                     0.8942               2
    1           6                       80%                     0.7410               3
    1           6                       70%                     0.9411               1
    1           6                       70%                     0.9140               2
    1           6                       70%                     0.7395               3

    2           4                      100%                     0.9533               1
    2           4                      100%                     0.9524               2
    2           4                      100%                     0.9523               3
    2           4                       80%                     0.9528               1
    2           4                       80%                     0.9514               2
    2           4                       80%                     0.9521               3
    2           4                       70%                     0.9528               1
    2           4                       70%                     0.9517               2
    2           4                       70%                     0.9517               3
    2           5                      100%                     0.9511               1
    2           5                      100%                     0.9496               2
    2           5                      100%                     0.9504               3
    2           5                       80%                     0.9496               1
    2           5                       80%                     0.9483               2
    2           5                       80%                     0.9481               3
    2           5                       70%                     0.9480               1
    2           5                       70%                     0.9488               2
    2           5                       70%                     0.9474               3
    2           6                      100%                     0.9446               1
    2           6                      100%                     0.9439               2
    2           6                      100%                     0.9433               3
    2           6                       80%                     0.9428               1
    2           6                       80%                     0.9437               2
    2           6                       80%                     0.9420               3
    2           6                       70%                     0.9428               1
    2           6                       70%                     0.9434               2
    2           6                       70%                     0.9432               3


I risultati sono stati effettuati fino alla decima partizione del training set.
Vista la dimensione del dataset scelto, è ragionevole circoscrivere gli esperimenti ad un numero di partizioni tra 2 e 10. 
Oltre la decima parte, la dimensione del training set locale è troppo piccola.
Ulteriori esperimenti possono essere effettuati se ritenuto interessante su altre macchine e/o cluster di macchine, con dataset diversi.

Per iterare per più round, magari fino a raggiungere una buona convergenza,
si potrebbe pensare di effettuare più round sugli stessi dati di training.
Alcuni esperimenti sono stati fatti in questo scenario, riporto alcuni risultati:

Training Set fissati
N. Round | N. Clients (tot) | Client selection per Round | Accuracy finale | N. Client Bizantini
    5           5                       100%                    0.9758               0
    5           5                       80%                     0.9762               0
    5           5                       70%                     0.9788               0
    5           7                       100%                    0.9733               0
    5           7                       80%                     0.9737               0
    5           7                       70%                     0.9727               0
    5           10                      100%                    0.9677               0
    5           10                      80%                     0.9682               0
    5           10                      70%                     0.9681               0

    5           5                      100%                     0.9736               1
    5           5                      100%                     0.9691               2
    5           5                      100%                     0.9321               3
    5           5                      100%                     0.7674               5
    5           5                       80%                     0.9738               1
    5           5                       80%                     0.9729               2
    5           5                       80%                     0.9579               3
    5           5                       80%                     0.7676               5
    5           5                       70%                     0.9759               1
    5           5                       70%                     0.9750               2
    5           5                       70%                     0.9658               3
    5           5                       70%                     0.7684               5
    5           7                      100%                     0.9718               1
    5           7                      100%                     0.9678               2
    5           7                      100%                     0.9589               3
    5           7                      100%                     0.7964               5
    5           7                       80%                     0.9689               1
    5           7                       80%                     0.9620               2
    5           7                       80%                     0.9608               3
    5           7                       80%                     0.7937               5
    5           7                       70%                     0.9688               1
    5           7                       70%                     0.9700               2
    5           7                       70%                     0.9701               3
    5           7                       70%                     0.9081               5
    5          10                      100%                     0.9676               1
    5          10                      100%                     0.9656               2
    5          10                      100%                     0.9625               3
    5          10                      100%                     0.9439               5
    5          10                       80%                     0.9657               1
    5          10                       80%                     0.9638               2
    5          10                       80%                     0.9610               3
    5          10                       80%                     0.9441               5
    5          10                       70%                     0.9662               1
    5          10                       70%                     0.9615               2
    5          10                       70%                     0.9519               3
    5          10                       70%                     0.9315               5

Le prestazioni del modello diminuiscono lentamente con la dimensione dei training set locali, pur mantenendo una buona accuracy.
Come ci si poteva aspettare, a parità di partizioni, sistemi con meno clients e più round performano meglio.

Analizzando le performance del sistema nelle varie configurazioni e considerando un trade-off tra accuracy e scalabilità,
e un setting (benchè semplice) quanto più verosimile a scenari reali di learning distribuito,
il numero ottimale di client tra i quali introdurre nodi bizantini sembra 5, 
allenati per 2 o 5 round rispettivamente per task real-time o a dati fissi.

Su queste configurazioni sono state effettuate analisi delle prestazioni all'introduzione di nodi bizantini.

L'implementazione scelta dei nodi bizantini riguarda la tipologia d'attacco considerata "default" in letteratura,
ovvero il _label flipping_: i nodi bizantini istanziati, appena prima del training,
invertono le label dei sample contenenti "1" e "7".
Si può aumentare il rumore invertendo altri sample, e si possono testare le performance del sistema 
introducendo nuovi tipi di nodi bizantini, che implementino attacchi di diverso tipo.

## Result #2:
Come ci si poteva aspettare, all'aumentare del numero di nodi bizantini l'accuracy diminuisce.
In particolare, segue le successioni:

<4 client, 1 round>
- (100% client per round): 0.9511 -> 0.9154 -> 0.7508 -> 0.7492 //diminuisce fortemente all'introduzione del secondo nodo bizantino,
- (80% client per round) : 0.9505 -> 0.8630 -> 0.7489 -> 0.7466 // diminuisce fortemente all'introduzione del primo e secondo nodo bizantino,
- (70% client per round) : 0.9510 -> 0.7511 -> 0.7511 -> 0.7511 // diminuisce all'introduzione del primo nodo e resta invariato dopo.

<4 client, 2 round>
- (100% client per round): 0.9539 -> 0.9533 -> 0.9524 -> 0.9523 // le prestazioni diminuiscono molto lentamente
- (80% client per round) : 0.9515 -> 0.9528 -> 0.9514 -> 0.9521 // le prestazioni diminuiscono molto lentamente 
- (70% client per round) : 0.9527 -> 0.9528 -> 0.9517 -> 0.9517 // le prestazioni diminuiscono molto lentamente

<5 client, 1 round>
- (100% client per round): 0.9474 -> 0.9277 -> 0.7883 -> 0.7462 // diminuisce fortemente all'introduzione del secondo nodo bizantino
- (80% client per round) : 0.9463 -> 0.9049 -> 0.7506 -> 0.7506 // diminuisce fortemente all'introduzione del secondo nodo bizantino
- (70% client per round) : 0.9479 -> 0.9455 -> 0.8440 -> 0.8572 // diminuisce fortemente all'introduzione del secondo nodo bizantino

<5 client, 2 round>
- (100% client per round): 0.9495 -> 0.9511 -> 0.9496 -> 0.9504 // le prestazioni diminuiscono molto lentamente o restano quasi invariate
- (80% client per round) : 0.9484 -> 0.9496 -> 0.9483 -> 0.9481 // le prestazioni diminuiscono molto lentamente o restano quasi invariate
- (70% client per round) : 0.9493 -> 0.9480 -> 0.9488 -> 0.9474 // le prestazioni diminuiscono molto lentamente o restano quasi invariate

<6 client, 1 round>
- (100% client per round): 0.9425 -> 0.9251 -> 0.8570 -> 0.7419 // diminuisce fortemente all'introduzione del secondo nodo bizantino
- (80% client per round) : 0.9405 -> 0.8997 -> 0.8942 -> 0.7410 // diminuisce all'introduzione del primo nodo bizantino e poi, più fortemente, all'introduzione del terzo 
- (70% client per round) : 0.9421 -> 0.9411 -> 0.9140 -> 0.7395 // diminuisce fortemente all'introduzione del terzo nodo bizantino

<6 client, 2 round>
- (100% client per round): 0.9452 -> 0.9446 -> 0.9439 -> 0.9433 // le prestazioni diminuiscono molto lentamente o restano quasi invariate
- (80% client per round):  0.9439 -> 0.9428 -> 0.9437 -> 0.9420 // le prestazioni diminuiscono molto lentamente o restano quasi invariate
- (70% client per round):  0.9427 -> 0.9428 -> 0.9434 -> 0.9432 // le prestazioni diminuiscono molto lentamente o restano quasi invariate

In generale, in sistemi con un singolo round, quando tutti i client vengono scelti ad ogni round, le prestazioni scendono in media di :
- ~0.02426666666 all'introduzione del primo nodo bizantino 
- ~0.12403333333 all'introduzione del secondo nodo bizantino
- ~0.05293333333 all'introduzione del terzo nodo bizantino

Le prestazioni del sistema risultano molto più stabili in configurazioni che comprendono almeno due round, con un cambiamento medio di:
- ~(-0.00013333333) all'introduzione del primo nodo bizantino 
- ~0.00103333333    all'introduzione del secondo nodo bizantino
- ~(-0.00003333333) all'introduzione del terzo nodo bizantino

Applicando una client selection dell'80% su sistemi a singolo round, si nota un effetto simile:
- ~0.05656666667    all'introduzione del primo nodo bizantino 
- ~0.0913           all'introduzione del secondo nodo bizantino
- ~0.05183333333    all'introduzione del terzo nodo bizantino

Infatti, le prestazioni risultano migliori aggiungendo il secondo round:
- ~(-0.0004666666667)   all'introduzione del primo nodo bizantino 
- ~0.0006               all'introduzione del secondo nodo bizantino
- ~0.0004               all'introduzione del terzo nodo bizantino

Stime simili si ottengono applicando una client selection del 70%, singolo round:
- ~0.06776666667    all'introduzione del primo nodo bizantino 
- ~0.04286666667    all'introduzione del secondo nodo bizantino
- ~0.05376666667    all'introduzione del terzo nodo bizantino

Doppio round:
- ~0.0003666666667  all'introduzione del primo nodo bizantino 
- ~(-0.0001)        all'introduzione del secondo nodo bizantino
- ~0.0005333333333  all'introduzione del terzo nodo bizantino

L'introduzione del secondo nodo bizantino sembra essere il momento più sensibile per le prestazioni del sistema.

La diminuizione del decadimento delle performance all'aumentare del numero di round, 
in sistemi che ricordiamo essere real-time (dunque il training set locale cambia ad ogni round),
potrebbe essere dovuto alla suddivisione del training set in più partizioni, 
che porta i nodi bizantini ad essere allenati su meno dati e attribuisce un peso minore ad ogni client.

Al diminuire del numero di client scelti ad ogni round, si nota un decadimento minore delle performance
in particolare all'introduzione del secondo nodo bizantino, che era stato evidenziato come il nodo che massimizzava il decadimento delle performance.

Le performance del sistema non sembrano risentire molto del numero di client totali del sistema, 
restando più o meno stabili, a parità del numero di nodi bizantini, al variare del numero totale di client.

Riportiamo di seguito le performance che riguardano sistemi con training set fissati e un numero maggiore di round:

<5, 100%>
0.9758 -> 0.9736 -> 0.9691 -> 0.9321 -> 0.7674
// Abbattimento lento delle performance per l'introduzione dei primi due nodi bizantini.
// Il terzo nodo abbatte più nettamente le performance, 
// che risulterebbero invece completamente schiacciate in presenza di 5 nodi bizantini (tutti i nodi del sistema)

5, 80%
0.9762 -> 0.9738 -> 0.9729 -> 0.9579 -> 0.7676
// Abbattimento lento delle performance per l'introduzione dei primi due nodi bizantini.
// Il terzo nodo abbatte più nettamente le performance, 
// che risulterebbero invece completamente schiacciate in presenza di 5 nodi bizantini (tutti i nodi del sistema)

5, 70%
0.9788 -> 0.9759 -> 0.9750 -> 0.9658 -> 0.7684
// Decadimento lento delle prestazioni, a meno del caso di 5/10 bizantini
// La client selection del 70% mitiga l'abbattimento delle performance all'introduzione del terzo nodo bizantino,
// portando ad un abbattimento di 0.0092, contro il 0.037 e il 0.015 dei due casi sopra.

7, 100%
0.9733 -> 0.9718 -> 0.9678 -> 0.9589 -> 0.7964
// Decadimento lento e costante delle performance
// Ancora una volta, l'abbattimento è preponderante quando si hanno 5 bizantini su 7 (più del 50%).

7, 80%
0.9737 -> 0.9689 -> 0.9620 -> 0.9608 -> 0.7937
// Decadimento lento, leggermente meno lento rispetto al client selection del 100%

7, 70%
0.9727 -> 0.9688 -> 0.9700 -> 0.9701 -> 0.9081
// Decadimento molto lento, che tende ad oscillare dopo l'introduzione del primo nodo bizantino.
// In questo caso, l'introduzione del quarto e quinto nodo bizantino porta a 
// un decadimento delle performance significativo, ma non rilevante quanto nei casi precedenti.
// Questa differenza potrebbe essere data da una selezione fortunata dei nodi ad ogni round.

10, 100%
0.9677 -> 0.9676 -> 0.9656 -> 0.9625 -> 0.9439
// Decadimento delle performance quasi nullo per i primi 3 nodi bizantini 

10, 80%
0.9682 -> 0.9657 -> 0.9638 -> 0.9610 -> 0.9441
// Decadimento delle performance quasi nullo per i primi 3 nodi bizantini 

10, 70%
0.9681 -> 0.9662 -> 0.9615 -> 0.9519 -> 0.9315
// Decadimento delle performance quasi nullo per i primi 2 nodi bizantini, che peggiora con l'introduzione del terzo nodo.

Questi dati confermano la maggior robustezza del sistema al crescere del numero di round,
che non solo quindi favoriscono prestazioni migliori del sistema, ma impedisce ai nodi bizantini di abbatterne l'accuracy.

Notiamo anche che, sebbene le prestazioni del sistema siano leggermente più basse
(decadimento dovuto sicuramente alla dimensione più bassa dei training set locali),
il sistema si comporta generalmente meglio al crescere di nodi bizantini quando vi sono più client totali.
Questo è particolarmente significativo all'introduzione di un numero alto (5/7 vs 5/10) di nodi bizantini,
in cui le performance passano da ~77% con 5-7 nodi totali a ~94% con 10 nodi totali.

Notiamo infine che anche in questo tipo di configurazione, 
la diminuizione del numero di client scelti ad ogni round mitiga generalmente 
l'abbattimento delle performance.

L'analisi di questi dati porta a selezionare una configurazione quanto più ideale 
per definire e testare un protocollo di consensus robusto ai nodi bizantini.
Le proprietà prese in considerazione sono:
- scalabilità   => n. nodi >= 5
- performance   => accuracy > 96%
- sensibilità ai nodi bizantini => abbattimento >= 0.002
- verosimiglianza dell'ambiente => n. round >= 5
Sulla base di queste assunzioni, la configurazione presa in esame è <5 client, 5 round, tr.set fissati>.

# FashionMNIST
{'hidden': 512, 'lr': 0.000362561763457623, 'batch_size': 50, 'epochs': 20} => 0.8821

python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8617
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 1 -b 1 -rtime -> Final global model accuracy: 0.8447
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 1 -b 2 -rtime -> Final global model accuracy: 0.6815
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 1 -b 3 -rtime -> Final global model accuracy: 0.6797
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8625
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.7601
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.6798
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.6796
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8568
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.6829
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.6801
python3 src/federated_learning.py -t fashionmnist -n 4 -r 1 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.6801
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8675
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 1 -b 1 -rtime -> Final global model accuracy: 0.864
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 1 -b 2 -rtime -> Final global model accuracy: 0.8651
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 1 -b 3 -rtime -> Final global model accuracy: 0.8609
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8653
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.8642
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.8624
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.8618
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8602
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.8598
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.6788
python3 src/federated_learning.py -t fashionmnist -n 4 -r 2 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.6788
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8603
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 1 -b 1 -rtime -> Final global model accuracy: 0.8531
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 1 -b 2 -rtime -> Final global model accuracy: 0.7026
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 1 -b 3 -rtime -> Final global model accuracy: 0.6785
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8584
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.842
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.6808
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.6768
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8583
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.8583
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.791
python3 src/federated_learning.py -t fashionmnist -n 5 -r 1 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.7457
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8643
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 1 -b 1 -rtime -> Final global model accuracy: 0.8644
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 1 -b 2 -rtime -> Final global model accuracy: 0.8619
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 1 -b 3 -rtime -> Final global model accuracy: 0.8623
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8633
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.8621
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.8626
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.8616
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8611
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.8611
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.8588
python3 src/federated_learning.py -t fashionmnist -n 5 -r 2 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.8634
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8538
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 1 -b 1 -rtime -> Final global model accuracy: 0.8483
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 1 -b 2 -rtime -> Final global model accuracy: 0.7577
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 1 -b 3 -rtime -> Final global model accuracy: 0.6762
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8551
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.8501
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.822
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.6764
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8551
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.821
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.8142
python3 src/federated_learning.py -t fashionmnist -n 6 -r 1 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.677
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 1 -b 0 -rtime -> Final global model accuracy: 0.8617
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 1 -b 1 -rtime -> Final global model accuracy: 0.8612
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 1 -b 2 -rtime -> Final global model accuracy: 0.8606
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 1 -b 3 -rtime -> Final global model accuracy: 0.8603
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.8 -b 0 -rtime -> Final global model accuracy: 0.8602
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.8 -b 1 -rtime -> Final global model accuracy: 0.8618
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.8 -b 2 -rtime -> Final global model accuracy: 0.859
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.8 -b 3 -rtime -> Final global model accuracy: 0.8588
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.7 -b 0 -rtime -> Final global model accuracy: 0.8598
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.7 -b 1 -rtime -> Final global model accuracy: 0.8602
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.7 -b 2 -rtime -> Final global model accuracy: 0.8582
python3 src/federated_learning.py -t fashionmnist -n 6 -r 2 -p 0.7 -b 3 -rtime -> Final global model accuracy: 0.8595


Fase 1
X Step 1. Scegliere dataset
X Step 2. Replicare architettura e risultati attesi 

Fase 2
X Step 1. Implementare lo stesso modello su un sistema distribuito (FL) in Ray e vedere i risultati
X Step 2. Introdurre nodi bizantini (modificando i dataset di quei nodi) e verificare le conseguenze nelle varie configurazioni del sistema.
          Sulla base di questi comportamenti determinare il/i settings migliori ai quali applicare il consensus
- Step 3. Introdurre protocollo di consensus
- Step 4. Effettuare test con metriche e scelte diverse
- Step 5. Introdurre BRA e altre tecniche di mitigazione per verificare la composizione di più meccanismi
