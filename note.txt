Fase 1
X Step 1. Scegliere dataset
X Step 2. Replicare architettura e risultati attesi 

Fase 2
X Step 1. Implementare lo stesso modello su un sistema distribuito (FL) in Ray e vedere i risultati
X Step 2. Introdurre nodi bizantini (modificando i dataset di quei nodi) e verificare le conseguenze nelle varie configurazioni del sistema.
          Sulla base di questi comportamenti determinare il/i settings migliori ai quali applicare il consensus
- Step 3. Introdurre protocollo di consensus
- Step 4. Effettuare test con metriche e scelte diverse
- Step 5. Introdurre BRA e altre tecniche di mitigazione per verificare la composizione di più meccanismi

# Idea:
L'idea del progetto è di implementare un semplice sistema di federated learning, composto da:
- un aggregator server;
- una decina di client.

Come usuale, il server manda l'architettura e i pesi iniziali della NN ai client che,
allenando il modello locale su una partizione del dataset, invieranno indietro al server il modello aggiornato.

L'idea per prevenire poisoning attacks e nodi bizantini, è quella di implementare un semplice algoritmo di consensus.
Il client, prima di mandare il modello aggiornato al server, lo manda ad altri K client. 
Questi dovranno votare il modello candidato, sulla base di metriche di similarità.
Se la maggioranza dei voti sarà positiva, il modello verrà mandato al server.

L'obiettivo finale del progetto è quello di effettuare esperimenti sulle metriche da utilizzare (similarità: euclidean distance [40], [52], cosine similarity [37], [38],
k-means [46], Pearson correlation coefficient [49], [51], etc.),
nonchè sul numero di client a cui mandare il modello (quanti client "validano" il modello locale di una macchina? Tutti? Solo quelli del sottoinsieme corrente? Uno solo del sottoinsieme corrente?), sulle varianti del protocollo di consensus e,
infine, su estensioni del sistema che utilizzino non solo metriche di similarità, 
ma anche metriche di performance, BRA (Krum aggregation,... ) e verification-based methods (vedi SVM e altri).

Nota: come evitare che il client modifichi il modello prima di mandarlo? magari il modello può essere cacheato dagli altri client e mandato da uno degli altri client in modo random, oppure può firmare con l'hash il risultato prima di inviarlo.

## Datasets:
l'idea iniziale era di utilizzare un dataset su malware android, che ben si sposa col progetto dato che:
- federated learning si può avere su un sistema distribuito nell'edge, quindi composto (o componibile) da smartphones;
- un attaccante potrebbe essere interessato a scalfire l'accuracy del modello per far passare un malware (magari in suo possesso) come goodware.
Inoltre, è sufficientemente grande da poterlo dividere tra i vari client.

Link dataset: https://archive.ics.uci.edu/dataset/855/tuandromd+(tezpur+university+android+malware+dataset)
Paper: https://www.semanticscholar.org/paper/97e4780810c69604f6e2f2f4e31b88bbef7ca669

Tuttavia, il dataset in questione è incompleto e sbilanciato nelle classi, per cui non è semplice allenare un modello in modo unbiased e ottenere delle buone prestazioni di partenza.
Per questo motivo, ho pensato di aggiungere un livello di astrazione, allenando un modello semplice sul dataset MNIST, che è ben conosciuto in letteratura e non richiede data preprocessing.
Modello e dataset sono in questo caso attori intercambiabili. Una volta che l'algoritmo è corretto, si può trovare un dataset più interessante e allenarvi sopra un modello più complesso.

## Result #0:
Ho preparato un sistema di ML classico sul dataset MNIST, con un modello MLP con 1 hidden layer di 512 neuroni.
Ho definito funzioni e procedure per allenare il modello sul dataset, effettuare una leggera model selection (ho ridotto al minimo il numero di parametri inseriti nella grid search: un paio di valori per le epoche,
per la batch-size, per il numero di neuroni nell'hidden layer e il learning rate), ri-allenare il modello scelto sull'intero training set e infine effettuare una valutazione del modello allenato.
Seguendo questo approccio, la model evaluation sul test set restituisce un'accuracy dello 0.9779 che, considerata l'architettura semplice del modello, i pochi parametri su cui sono state valutate le performances,
il basso numero di epoche e il mancato utilizzo di tecniche più avanzate (drop-out, RNN, weight-decay, ...) o altri tipi di ottimizzatori, è un ottimo risultato iniziale.
Il sistema in questione è stato scritto utilizzando PyTorch e Ray Tune.

## Result #1:
Ho replicato un semplice sistema di Federated Learning su cui effettuare esperimenti.
La client selection avviene selezionando k <= n client in modo casuale, con k parametro scelto dall'utente.
La model aggregation viene effettuata calcolando la media tra i k modelli locali.

Come prima sperimentazione, ho istanziato diversi numeri di client su un sistema di apprendimento a round singolo.
Un test sull'affidabilità e la replicabilità dei dati è stato effettuato su un sistema con un solo client, riportando l'accuracy attesa.
I risultati sono qui riportati:

Training set dinamici (real-time)
N. Clients (tot) | N. Round | Client selection per Round | Accuracy finale | N. Client Bizantini
        1             1                     100%                     0.9779               0
        1             1                     100%                     0.7674               1
        2             1                     100%                     0.9672               0
        2             1                     100%                     0.7678               1
        3             1                     100%                     0.9588               0
        4             1                     100%                     0.9511               0
        4             1                     80%                      0.9505               0
        4             1                     70%                      0.9510               0
        5             1                     100%                     0.9474               0
        5             1                     80%                      0.9463               0
        5             1                     70%                      0.9479               0
        6             1                     100%                     0.9425               0
        6             1                     80%                      0.9405               0
        6             1                     70%                      0.9421               0
        7             1                     100%                     0.9396               0
        8             1                     100%                     0.9377               0
        9             1                     100%                     0.9327               0
        10            1                     100%                     0.9291               0
        2             2                     100%                     0.9688               0
        3             2                     100%                     0.9621               0
        4             2                     100%                     0.9539               0
        4             2                     80%                      0.9515               0
        4             2                     70%                      0.9527               0
        5             2                     100%                     0.9495               0
        5             2                     80%                      0.9484               0
        5             2                     70%                      0.9493               0
        6             2                     100%                     0.9452               0
        6             2                     80%                      0.9439               0
        6             2                     70%                      0.9427               0
        2             3                     100%                     0.9678               0
        3             3                     100%                     0.9607               0
        4             3                     100%                     0.9537               0
        4             1                     100%                     0.9154               1
        4             1                     100%                     0.7508               2
        4             1                     100%                     0.7492               3
        4             1                      80%                     0.8630               1
        4             1                      80%                     0.7489               2
        4             1                      80%                     0.7466               3
        4             1                      70%                     0.7511               1
        4             1                      70%                     0.7511               2
        4             1                      70%                     0.7511               3
        5             1                     100%                     0.9277               1
        5             1                     100%                     0.7883               2
        5             1                     100%                     0.7462               3
        5             1                      80%                     0.9049               1
        5             1                      80%                     0.7506               2
        5             1                      80%                     0.7506               3
        5             1                      70%                     0.9455               1
        5             1                      70%                     0.8440               2
        5             1                      70%                     0.8572               3
        6             1                     100%                     0.9251               1
        6             1                     100%                     0.8570               2
        6             1                     100%                     0.7419               3
        6             1                      80%                     0.8997               1
        6             1                      80%                     0.8942               2
        6             1                      80%                     0.7410               3
        6             1                      70%                     0.9411               1
        6             1                      70%                     0.9140               2
        6             1                      70%                     0.7395               3
        4             2                     100%                     0.9533               1
        4             2                     100%                     0.9524               2
        4             2                     100%                     0.9523               3
        4             2                      80%                     0.9528               1
        4             2                      80%                     0.9514               2
        4             2                      80%                     0.9521               3
        4             2                      70%                     0.9528               1
        4             2                      70%                     0.9517               2
        4             2                      70%                     0.9517               3
        5             2                     100%                     0.9511               1
        5             2                     100%                     0.9496               2
        5             2                     100%                     0.9504               3
        5             2                      80%                     0.9496               1
        5             2                      80%                     0.9483               2
        5             2                      80%                     0.9481               3
        5             2                      70%                     0.9480               1
        5             2                      70%                     0.9488               2
        5             2                      70%                     0.9474               3
        6             2                     100%                     0.9446               1
        6             2                     100%                     0.9439               2
        6             2                     100%                     0.9433               3
        6             2                      80%                     0.9428               1
        6             2                      80%                     0.9437               2
        6             2                      80%                     0.9420               3
        6             2                      70%                     0.9428               1
        6             2                      70%                     0.9434               2
        6             2                      70%                     0.9432               3


I risultati sono stati effettuati fino alla decima partizione del training set.
Vista la dimensione del dataset scelto, è ragionevole circoscrivere gli esperimenti ad un numero di partizioni tra 2 e 10. 
Oltre la decima parte, la dimensione del training set locale è troppo piccola.
Ulteriori esperimenti possono essere effettuati se ritenuto interessante su altre macchine e/o cluster di macchine, con dataset diversi.

Per iterare per più round, magari fino a raggiungere una buona convergenza,
si potrebbe pensare di effettuare più round sugli stessi dati di training.
Alcuni esperimenti sono stati fatti in questo scenario, riporto alcuni risultati:

Training Set fissati
N. Clients (tot) | N. Round | Client selection per Round | Accuracy finale | N. Client Bizantini
    5                 5               100%                    0.9758               0
    5                 5               80%                     0.9762               0
    5                 5               70%                     0.9788               0
    7                 5               100%                    0.9733               0
    7                 5               80%                     0.9737               0
    7                 5               70%                     0.9727               0
    10                5               100%                    0.9677               0
    10                5               80%                     0.9682               0
    10                5               70%                     0.9681               0
    5                 5              100%                     0.9736               1
    5                 5              100%                     0.9691               2
    5                 5              100%                     0.9321               3
    5                 5              100%                     0.7674               5
    5                 5               80%                     0.9738               1
    5                 5               80%                     0.9729               2
    5                 5               80%                     0.9579               3
    5                 5               80%                     0.7676               5
    5                 5               70%                     0.9759               1
    5                 5               70%                     0.9750               2
    5                 5               70%                     0.9658               3
    5                 5               70%                     0.7684               5
    7                 5              100%                     0.9718               1
    7                 5              100%                     0.9678               2
    7                 5              100%                     0.9589               3
    7                 5              100%                     0.7964               5
    7                 5               80%                     0.9689               1
    7                 5               80%                     0.9620               2
    7                 5               80%                     0.9608               3
    7                 5               80%                     0.7937               5
    7                 5               70%                     0.9688               1
    7                 5               70%                     0.9700               2
    7                 5               70%                     0.9701               3
    7                 5               70%                     0.9081               5
    10                5              100%                     0.9676               1
    10                5              100%                     0.9656               2
    10                5              100%                     0.9625               3
    10                5              100%                     0.9439               5
    10                5               80%                     0.9657               1
    10                5               80%                     0.9638               2
    10                5               80%                     0.9610               3
    10                5               80%                     0.9441               5
    10                5               70%                     0.9662               1
    10                5               70%                     0.9615               2
    10                5               70%                     0.9519               3
    10                5               70%                     0.9315               5

Le prestazioni del modello diminuiscono lentamente con la dimensione dei training set locali, pur mantenendo una buona accuracy.
Come ci si poteva aspettare, a parità di partizioni, sistemi con meno clients e più round performano meglio.

Analizzando le performance del sistema nelle varie configurazioni e considerando un trade-off tra accuracy e scalabilità,
e un setting (benchè semplice) quanto più verosimile a scenari reali di learning distribuito,
il numero ottimale di client tra i quali introdurre nodi bizantini sembra 5, 
allenati per 2 o 5 round rispettivamente per task real-time o a dati fissi.

Su queste configurazioni sono state effettuate analisi delle prestazioni all'introduzione di nodi bizantini.

L'implementazione scelta dei nodi bizantini riguarda la tipologia d'attacco considerata "default" in letteratura,
ovvero il _label flipping_: i nodi bizantini istanziati, appena prima del training,
invertono le label dei sample contenenti "1" e "7".
Si può aumentare il rumore invertendo altri sample, e si possono testare le performance del sistema 
introducendo nuovi tipi di nodi bizantini, che implementino attacchi di diverso tipo.

## Result #2:
Come ci si poteva aspettare, all'aumentare del numero di nodi bizantini l'accuracy diminuisce.
In particolare, segue le successioni:

<Client selection 100%>:
4 clients, 1 rounds: 0.9511 -> 0.9154 -> 0.7508 -> 0.7492
diminuisce fortemente all'introduzione del secondo nodo bizantino

4 clients, 2 rounds: 0.9539 -> 0.9533 -> 0.9524 -> 0.9523
le prestazioni diminuiscono molto lentamente

5 clients, 1 rounds: 0.9474 -> 0.9277 -> 0.7883 -> 0.7462
diminuisce fortemente all'introduzione del secondo nodo bizantino

5 clients, 2 rounds: 0.9495 -> 0.9511 -> 0.9496 -> 0.9504
le prestazioni diminuiscono molto lentamente o restano quasi invariate

6 clients, 1 rounds: 0.9425 -> 0.9251 -> 0.857 -> 0.7419
diminuisce fortemente all'introduzione del secondo nodo bizantino

6 clients, 2 rounds: 0.9452 -> 0.9446 -> 0.9439 -> 0.9433
le prestazioni diminuiscono molto lentamente o restano quasi invariate


<Client Selection 80%>:
4 clients, 1 rounds: 0.9505 -> 0.863 -> 0.7489 -> 0.7466
diminuisce fortemente all'introduzione del primo e secondo nodo bizantino,

4 clients, 2 rounds: 0.9515 -> 0.9528 -> 0.9514 -> 0.9521
le prestazioni diminuiscono molto lentamente 

5 clients, 1 rounds: 0.9463 -> 0.9049 -> 0.7506 -> 0.7506
le prestazioni diminuiscono molto lentamente 

5 clients, 2 rounds: 0.9484 -> 0.9496 -> 0.9483 -> 0.9481
le prestazioni diminuiscono molto lentamente 

6 clients, 1 rounds: 0.9405 -> 0.8997 -> 0.8942 -> 0.741
diminuisce all'introduzione del primo nodo bizantino e poi, più fortemente, all'introduzione del terzo 

6 clients, 2 rounds: 0.9439 -> 0.9428 -> 0.9437 -> 0.942
le prestazioni diminuiscono molto lentamente o restano quasi invariate

<Client Selection 70%>:
4 clients, 1 rounds: 0.951 -> 0.7511 -> 0.7511 -> 0.7511
diminuisce all'introduzione del primo nodo e resta invariato dopo.

4 clients, 2 rounds: 0.9527 -> 0.9528 -> 0.9517 -> 0.9517
le prestazioni diminuiscono molto lentamente

5 clients, 1 rounds: 0.9479 -> 0.9455 -> 0.844 -> 0.8572
diminuisce fortemente all'introduzione del secondo nodo bizantino

5 clients, 2 rounds: 0.9493 -> 0.948 -> 0.9488 -> 0.9474
le prestazioni diminuiscono molto lentamente o restano quasi invariate

6 clients, 1 rounds: 0.9421 -> 0.9411 -> 0.914 -> 0.7395
diminuisce fortemente all'introduzione del terzo nodo bizantino

6 clients, 2 rounds: 0.9427 -> 0.9428 -> 0.9434 -> 0.9432
le prestazioni diminuiscono molto lentamente o restano quasi invariate

In generale, l'introduzione del secondo nodo bizantino sembra essere il momento più sensibile per le prestazioni del sistema,
quando cioè il numero di bizantini è tra il 33.33% e il 50% dei nodi totali.
Le prestazioni del sistema risultano molto più stabili in configurazioni che comprendono almeno due round.

La diminuizione del decadimento delle performance all'aumentare del numero di round, 
in sistemi real-time (dove dunque il training set locale cambia ad ogni round),
potrebbe essere dovuto alla suddivisione del training set in più partizioni, 
che porta i nodi bizantini ad essere allenati su meno dati e attribuisce un peso minore ad ogni client.

Analizzando i grafici e mettendo in correlazione le curve di sistemi a singolo round 
con quelle di sistemi a doppio round, appare evidente che, 
mentre nel primo caso le prestazioni sono molto simili e calano velocemente, 
nel secondo caso le prestazioni sono leggermente differenti in sistemi in stato normale (0 bizantini),
e decrescono con una tendenza più smussata. Questo conferma la robustezza introdotta dal secondo round.

Per quanto riguarda la client selection in queste configurazioni, sembra che le variazioni delle prestazioni 
varino da scenario a scenario. In particolare, una model selection del 70% sembra funzionare meglio 
in sistemi a singolo round e con 5-6 client, mentre non sembra aiutare in sistemi a doppio round.

Riportiamo di seguito le performance che riguardano sistemi con training set fissati e un numero maggiore di round:

<Client selection 100%>:
5 clients, 5 rounds: 0.9758 -> 0.9736 -> 0.9691 -> 0.9321
Abbattimento lento delle performance per l'introduzione dei primi due nodi bizantini.
Il terzo nodo abbatte più nettamente le performance, 
che risulterebbero invece completamente schiacciate in presenza di 5 nodi bizantini (tutti i nodi del sistema)

7 clients, 5 rounds: 0.9733 -> 0.9718 -> 0.9678 -> 0.9589
Decadimento lento e costante delle performance
Ancora una volta, l'abbattimento è preponderante quando si hanno 5 bizantini su 7 (più del 50%).

10 clients, 5 rounds: 0.9677 -> 0.9676 -> 0.9656 -> 0.9625
Decadimento delle performance quasi nullo per i primi 3 nodi bizantini 

<Client Selection 80%>:
5 clients, 5 rounds: 0.9762 -> 0.9738 -> 0.9729 -> 0.9579
Abbattimento lento delle performance per l'introduzione dei primi due nodi bizantini.
Il terzo nodo abbatte più nettamente le performance, 
che risulterebbero invece completamente schiacciate in presenza di 5 nodi bizantini (tutti i nodi del sistema)

7 clients, 5 rounds: 0.9737 -> 0.9689 -> 0.962 -> 0.9608
Decadimento lento, leggermente meno lento rispetto al client selection del 100%

10 clients, 5 rounds: 0.9682 -> 0.9657 -> 0.9638 -> 0.961
Decadimento delle performance quasi nullo per i primi 3 nodi bizantini 

<Client Selection 70%>:

5 clients, 5 rounds: 0.9788 -> 0.9759 -> 0.975 -> 0.9658
Decadimento lento delle prestazioni, a meno del caso di 5/10 bizantini
La client selection del 70% mitiga l'abbattimento delle performance all'introduzione del terzo nodo bizantino,
portando ad un abbattimento di 0.0092, contro il 0.037 e il 0.015 dei due casi sopra.

7 clients, 5 rounds: 0.9727 -> 0.9688 -> 0.97 -> 0.9701
Decadimento molto lento, che tende ad oscillare dopo l'introduzione del primo nodo bizantino.
In questo caso, l'introduzione del quarto e quinto nodo bizantino porta a 
un decadimento delle performance significativo, ma non rilevante quanto nei casi precedenti.
Questa differenza potrebbe essere data da una selezione fortunata dei nodi ad ogni round.

10 clients, 5 rounds: 0.9681 -> 0.9662 -> 0.9615 -> 0.9519
Decadimento delle performance quasi nullo per i primi 2 nodi bizantini, che peggiora con l'introduzione del terzo nodo.

Questi dati confermano la maggior robustezza del sistema al crescere del numero di round,
che non solo quindi favoriscono prestazioni migliori del sistema, ma impedisce ai nodi bizantini di abbatterne l'accuracy.

Notiamo anche che, sebbene le prestazioni del sistema siano leggermente più basse
(decadimento dovuto sicuramente alla dimensione più bassa dei training set locali),
il sistema si comporta generalmente meglio quando il numero di nodi totali è più alto,
diminuendo di fatto la proporzione tra nodi benigni e bizantini.

Questo è confermato da ulteriori analisi fatte aumentando il numero di nodi bizantini a 5:
portando la proporzione di bizantini per nodi totali tra il 50% e il 71,43%,
il sistema continua a performare bene con un numero maggiore di nodi totali. 
Le performance passano in questi casi da ~77% con 5-7 nodi totali a ~94% con 10 nodi totali.
Questo suggerisce che sistemi con più nodi siano in generale più robusti 

Notiamo infine che in questo tipo di configurazione, la client selection più stretta (70%) 
migliora le prestazioni in sistemi con 5-7 clients, mentre le peggiora in presenza di 10 client.

## Result #3
Una seconda analisi è stata effettuata su un altro dataset, FashionMNIST.
Essendo questo dataset più complesso di MNIST, è stato utilizzato un modello più complesso rispetto all'MLP 
usato precedentemente. Una CNN è stata allenata su hyperparameters trovati tramite model selection.
Gli hyperparameters sono {'lr': 0.0006251373574521746, 'batch_size': 128, 'epochs': 10},
e la performance finale del modello raggiunge un'accuracy del 0.9088.
Le due analisi vedono quindi modelli selezionati tramite una rigorosa fase di model selection, 
utilizzando modelli di complessità crescente in base al problema. 

Real-Time
| Total Clients | Rounds | Clients per Round  | Accuracy | Byzantine Clients
        4            1            100%           0.9027         0
        4            1            80%            0.9006         0
        4            1            70%            0.8908         0
        4            2            100%           0.9012         0
        4            2            80%            0.8997         0
        4            2            70%            0.8960         0
        5            1            100%           0.9005         0
        5            1            80%            0.8987         0
        5            1            70%            0.8968         0
        5            2            100%           0.8978         0
        5            2            80%            0.8974         0
        5            2            70%            0.8938         0
        6            1            100%           0.8976         0
        6            1            80%            0.8968         0
        6            1            70%            0.8953         0
        6            2            100%           0.8940         0
        6            2            80%            0.8922         0
        6            2            70%            0.8938         0
        4            1            100%           0.8982         1
        4            1            80%            0.8600         1
        4            1            70%            0.7664         1
        4            2            100%           0.9023         1
        4            2            80%            0.8966         1
        4            2            70%            0.8975         1
        5            1            100%           0.8987         1
        5            1            80%            0.8844         1
        5            1            70%            0.8978         1
        5            2            100%           0.9010         1
        5            2            80%            0.8982         1
        5            2            70%            0.8972         1
        6            1            100%           0.8949         1
        6            1            80%            0.8793         1
        6            1            70%            0.8790         1
        6            2            100%           0.8958         1
        6            2            80%            0.8932         1
        6            2            70%            0.8981         1
        4            1            100%           0.7323         2
        4            1            80%            0.7121         2
        4            1            70%            0.7071         2
        4            2            100%           0.9028         2
        4            2            80%            0.8963         2
        4            2            70%            0.8982         2
        5            1            100%           0.8057         2
        5            1            80%            0.7584         2
        5            1            70%            0.8621         2
        5            2            100%           0.8990         2
        5            2            80%            0.9002         2
        5            2            70%            0.8962         2
        6            1            100%           0.8666         2
        6            1            80%            0.8790         2
        6            1            70%            0.8913         2
        6            2            100%           0.8923         2
        6            2            80%            0.8953         2
        6            2            70%            0.8940         2
        4            1            100%           0.7110         3
        4            1            80%            0.7096         3
        4            1            70%            0.7090         3
        4            2            100%           0.9027         3
        4            2            80%            0.8974         3
        4            2            70%            0.8948         3
        5            1            100%           0.7099         3
        5            1            80%            0.7113         3
        5            1            70%            0.8298         3
        5            2            100%           0.8981         3
        5            2            80%            0.8996         3
        5            2            70%            0.8957         3
        6            1            100%           0.7117         3
        6            1            80%            0.7222         3
        6            1            70%            0.7069         3
        6            2            100%           0.8922         3
        6            2            80%            0.8909         3
        6            2            70%            0.8881         3

Training set fissati
| N. Clients | N. Rounds | Client Selection |    Final Accuracy | Byzantine Clients
| -----------| ----------| -----------------| ------------------| -------------- |
        5          5            100%                0.914               0
        5          5            80%                 0.913               0
        5          5            70%                 0.9132              0
        7          5            100%                0.9117              0
        7          5            80%                 0.91                0
        7          5            70%                 0.9118              0
        10         5            100%                0.9101              0
        10         5            80%                 0.9067              0
        10         5            70%                 0.9078              0
        5          5            100%                0.9104              1
        5          5            80%                 0.9092              1
        5          5            70%                 0.9084              1
        7          5            100%                0.9108              1
        7          5            80%                 0.9093              1
        7          5            70%                 0.9058              1
        10         5            100%                0.9115              1
        10         5            80%                 0.9072              1
        10         5            70%                 0.9094              1
        5          5            100%                0.8976              2
        5          5            80%                 0.908               2
        5          5            70%                 0.9096              2
        7          5            100%                0.9042              2
        7          5            80%                 0.9039              2
        7          5            70%                 0.8959              2
        10         5            100%                0.9062              2
        10         5            80%                 0.9045              2
        10         5            70%                 0.9028              2
        5          5            100%                0.7859              3
        5          5            80%                 0.8727              3
        5          5            70%                 0.9011              3
        7          5            100%                0.8955              3
        7          5            80%                 0.9018              3
        7          5            70%                 0.9049              3
        10         5            100%                0.9063              3
        10         5            80%                 0.8932              3
        10         5            70%                 0.8986              3

<Client selection 100%>:
4 clients, 1 rounds: 0.9027 -> 0.8982 -> 0.7323 -> 0.711
L'abbattimento maggiore delle performance si ha introducento il secondo nodo bizantino

4 clients, 2 rounds: 0.9012 -> 0.9023 -> 0.9028 -> 0.9027
Abbattimento praticamente nullo

5 clients, 1 rounds: 0.9005 -> 0.8987 -> 0.8057 -> 0.7099
Abbattimento delle performance minore rispetto al caso precedente ma comunque significativo: 
9% all'introduzione del secondo nodo bizantino, 10% al terzo.

5 clients, 2 rounds: 0.8978 -> 0.901 -> 0.899 -> 0.8981
Abbattimento praticamente nullo 

6 clients, 1 rounds: 0.8976 -> 0.8949 -> 0.8666 -> 0.7117
Sistema più robusto: solo al terzo nodo bizantino si ha un abbattimento significativo delle performance.

6 clients, 2 rounds: 0.894 -> 0.8958 -> 0.8923 -> 0.8922
Abbattimento praticamente nullo

<Client Selection 80%>:
4 clients, 1 rounds: 0.9006 -> 0.86 -> 0.7121 -> 0.7096
L'abbattimento maggiore delle performance si ha introducento il secondo nodo bizantino

4 clients, 2 rounds: 0.8997 -> 0.8966 -> 0.8963 -> 0.8974
Abbattimento praticamente nullo

5 clients, 1 rounds: 0.8987 -> 0.8844 -> 0.7584 -> 0.7113
L'abbattimento maggiore delle performance si ha introducento il secondo nodo bizantino

5 clients, 2 rounds: 0.8974 -> 0.8982 -> 0.9002 -> 0.8996
Abbattimento praticamente nullo

6 clients, 1 rounds: 0.8968 -> 0.8793 -> 0.879 -> 0.7222
Sistema più robusto: solo al terzo nodo bizantino si ha un abbattimento significativo delle performance.

6 clients, 2 rounds: 0.8922 -> 0.8932 -> 0.8953 -> 0.8909
Abbattimento praticamente nullo

<Client Selection 70%>:
4 clients, 1 rounds: 0.8908 -> 0.7664 -> 0.7071 -> 0.709
L'abbattimento maggiore delle performance si ha introducento il secondo nodo bizantino

4 clients, 2 rounds: 0.896 -> 0.8975 -> 0.8982 -> 0.8948
Abbattimento praticamente nullo

5 clients, 1 rounds: 0.8968 -> 0.8978 -> 0.8621 -> 0.8298
L'abbattimento maggiore delle performance si ha introducento il secondo nodo bizantino

5 clients, 2 rounds: 0.8938 -> 0.8972 -> 0.8962 -> 0.8957
Abbattimento praticamente nullo

6 clients, 1 rounds: 0.8953 -> 0.879 -> 0.8913 -> 0.7069
Sistema più robusto. 
Solo al terzo nodo bizantino si ha un abbattimento significativo delle performance.

6 clients, 2 rounds: 0.8938 -> 0.8981 -> 0.894 -> 0.8881
Abbattimento praticamente nullo

In questo nuovo scenario, sembra che l'impatto dovuto all'introduzione del secondo nodo bizantino sia rilevante solo su sistemi
con 4 e 5 client, mentre non è significativo (anzi, le performance sembrano aumentare leggermente) in presenza di 6 clients,
dove cioè la proporzione di bizantini scende dal 40 al 33.33%, in nessuna delle tre configurazioni 
(che differiscono in quanto alla percentuale della client selection).

Di nuovo, le prestazioni del sistema risultano molto più stabili in configurazioni che comprendono almeno due round,
con un decadimento quasi nullo in tutte le configurazioni.

Come per MNIST, notiamo che le prestazioni dei sistemi si distaccano con più round, 
mentre seguono circa la stessa tendenza in configurazioni a singolo round.

Riportiamo di seguito le performance che riguardano sistemi con training set fissati e un numero maggiore di round:

<Client selection 100%>:
5 clients, 5 rounds: 0.914 -> 0.9104 -> 0.8976 -> 0.7859
Abbattimento nullo con 1 bizantino, basso al secondo, alto al terzo.

7 clients, 5 rounds: 0.9117 -> 0.9108 -> 0.9042 -> 0.8955
Abbattimento nullo con 1 bizantino, basso al secondo e al terzo.

10 clients, 5 rounds: 0.9101 -> 0.9115 -> 0.9062 -> 0.9063
Abbattimento nullo con 1 bizantino, basso al secondo e di nuovo nullo al terzo.

<Client Selection 80%>:
5 clients, 5 rounds: 0.913 -> 0.9092 -> 0.908 -> 0.8727
Abbattimento delle prestazioni basso ad ogni nodo introdotto.

7 clients, 5 rounds: 0.91 -> 0.9093 -> 0.9039 -> 0.9018
Abbattimento delle prestazioni quasi nullo

10 clients, 5 rounds: 0.9067 -> 0.9072 -> 0.9045 -> 0.8932
Abbattimneto delle prestazioni quasi nullo. ~1% con 3 nodi bizantini

<Client Selection 70%>:
5 clients, 5 rounds: 0.9132 -> 0.9084 -> 0.9096 -> 0.9011
Abbattimento quasi nullo

7 clients, 5 rounds: 0.9118 -> 0.9058 -> 0.8959 -> 0.9049
Abbattimento quasi nullo

10 clients, 5 rounds: 0.9078 -> 0.9094 -> 0.9028 -> 0.8986
Abbattimento quasi nullo

Questi dati confermano la maggior robustezza del sistema al crescere del numero di round,
che non solo quindi favoriscono prestazioni migliori del sistema, ma impedisce ai nodi bizantini di abbatterne l'accuracy.

Notiamo anche che, sebbene le prestazioni del sistema siano leggermente più basse
(decadimento dovuto sicuramente alla dimensione più bassa dei training set locali),
il sistema si comporta generalmente meglio quando il numero di nodi totali è più alto,
diminuendo di fatto la proporzione tra nodi benigni e bizantini.

In questo caso, notiamo che la diminuizione del numero di client scelti ad ogni round non porta 
ad una mitigazione dell'impatto dei nodi bizantini, ma ad un'amplificazione della stessa.
Questo può in generale essere dovuto dal coinvolgimento di una componente pseudo-randomica all'interno
della client selection, che potrebbe selezionare di volta in volta i nodi sbagliati (bizantini).

Le analisi sui due sistemi sono poi state aggregate per visualizzare in modo più immediato
il comportamento dei sistemi in funzione dei nodi bizantini.
Questi grafici confermano le considerazioni individuali fatte sui due modelli, 
evidenziando tra le altre cose la criticità della client selection per le performance del sistema.

L'aggregazione dei dati sui due task ci permette di validare la somiglianza di risultati sui due dataset,
portandoci a concludere che le osservazioni e le considerazioni fatte sui due esperimenti abbiano dei 
fondamenti sistematici che vale la pena indagare e approfondire, facendo nuovi esperimenti su questi e altri 
dataset, con più nodi, più round e diversi tipi di client selection, 
per progettare un sistema di federated learning performante e robusto.

I risultati di queste analisi e dei confronti tra le prestazioni dei sistemi al variare delle configurazioni,
permettono di valutare la robustezza di sistemi di Federated Learning, al fine di identificare l'architettura
più byzantine-resilient. I dati suggeriscono un'architettura che comprenda:
- Un alto numero di nodi >= 6;
- Un numero di round >= 2;
- Un processo di client selection più complesso, selezionato e definito in base agli altri parametri
  del modello. La percentuale potrebbe essere considerata un iperparametro del modello distribuito.
  In generale, una percentuale di client selection bassa può essere rischiosa in presenza di nodi bizantini.

Visti i risultati delle analisi, si potrebbe, anche in istanze real-time, 
iterare per più rounds sugli stessi training set prima di cambiarli.
Questo porterebbe probabilmente un aumento generale delle prestazioni e una maggiore resistenza ai bizantini.
Questi dati forniscono di fatto le considerazioni iniziali sulle quali basare una prima versione del nostro protocollo di consensus.